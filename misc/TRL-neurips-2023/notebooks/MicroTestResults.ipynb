{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.contingency_tables as ct\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_data_path = \"../resources/BenchmarkDataset\"\n",
    "benchmark_dataset = os.listdir(benchmark_data_path)\n",
    "# ---> final micro results with HTML formats\n",
    "benchmark_save_path2 = \"../resources/BenchmarkResults/BenchmarkResults_21_9_2023_new_save_format_HTMLFormat_redo/\"\n",
    "# ---> final micro results with 6 format\n",
    "benchmark_save_path1 = \"../resources/BenchmarkResults/BenchmarkResults_21_9_2023_new_save_format_redo/\"\n",
    "benchmark_dataset = [data for data in benchmark_dataset if data.endswith(\n",
    "    \".csv\") and \"breast\" not in data]\n",
    "dfs = []\n",
    "for bd in benchmark_dataset:\n",
    "\n",
    "    pass_metrics = []\n",
    "    print(bd)\n",
    "    df1 = pd.read_csv(os.path.join(benchmark_save_path1, bd,\n",
    "                      \"Micro_\"+bd+\"_output_revamped.csv\"))\n",
    "    try:\n",
    "        df2 = pd.read_csv(os.path.join(benchmark_save_path2,\n",
    "                          bd, \"Micro_\"+bd+\"_output_revamped.csv\"))\n",
    "        df = pd.concat([df1, df2], ignore_index=True)\n",
    "    except:\n",
    "        df = df1\n",
    "    print(df1.shape, df2.shape, df.shape)\n",
    "    df[\"Dataset-name\"] = bd\n",
    "    dfs.append(df)\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "pivot_table = all_data.pivot_table(\n",
    "    index=[\"TestCase\", 'TableManipulation', \"temperature\"],\n",
    "    columns='tableFormat',\n",
    "    values=['pass_1', 'pass_3', 'pass_5', 'pass_10', 'pass_15', 'Result'],\n",
    "    # Custom aggregation function to append values to a list\n",
    "    aggfunc=lambda x: list(x)\n",
    ")\n",
    "attributes = {c: list(df[c].unique()) for c in df.columns if c in [\n",
    "    \"temperature\", \"tableFormat\", \"TableManipulation\", \"TestCase\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pass @ 1, temp =0 Tests Vs formats across all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_test_vs_format_on_original_data = all_data[(\n",
    "    all_data[\"TableManipulation\"] == \"OriginalData\") & (all_data[\"temperature\"] == 0.0)]\n",
    "test_vs_format_on_original_data = all_data_test_vs_format_on_original_data.pivot_table(\n",
    "    index=[\"TestCase\"],\n",
    "    columns='tableFormat',\n",
    "    values=['pass_1'],\n",
    "    aggfunc=[\"mean\"]  # Custom aggregation function to append values to a list\n",
    ")\n",
    "test_vs_format_on_original_data = np.round(\n",
    "    test_vs_format_on_original_data, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_original_data.T.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-value RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "data_id = all_data[all_data[\"TableManipulation\"].isin([\"OriginalData\"]) & (\n",
    "    all_data[\"temperature\"] == 0.0)] \n",
    "pivot_table_across_formats = data_id.pivot_table(\n",
    "    index=[\"TestCase\"],\n",
    "    columns='tableFormat',\n",
    "    values=['pass_1'],\n",
    "    # Custom aggregation function to append values to a list\n",
    "    aggfunc=lambda x: list(x)\n",
    ")\n",
    "p_val = {}\n",
    "for id in range(pivot_table_across_formats.shape[0]):\n",
    "    print(id)\n",
    "    print(f\"For Test: {pivot_table_across_formats.index[id]} pass@1 temp =0.1\")\n",
    "    sorted_list = [i[-1] for i in test_vs_format_on_original_data.T.sort_values(\n",
    "        by=[pivot_table_across_formats.index[id]], ascending=False).index]\n",
    "    sub_vals = {}\n",
    "    index_val = pivot_table_across_formats.index[id]\n",
    "    for x in range(1, len(sorted_list)):\n",
    "        vals1 = pivot_table_across_formats.loc[index_val,\n",
    "                                               (\"pass_1\", sorted_list[0])]\n",
    "        vals2 = pivot_table_across_formats.loc[index_val,\n",
    "                                               (\"pass_1\", sorted_list[x])]\n",
    "        min_val = min(len(vals1), len(vals2))\n",
    "        print(min_val)\n",
    "        p_value = scipy.stats.ttest_rel(vals1[:min_val], vals2[:min_val])\n",
    "        sub_vals[f\"{sorted_list[0]}-{sorted_list[x]}\"] = {\"p-value\": p_value.pvalue,\n",
    "                                                          \"statistics\": p_value.statistic, \"df\": p_value.df, \"tests_count\": min_val}\n",
    "    p_val[pivot_table_across_formats.index[id]] = sub_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, pd.Series):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, datetime):\n",
    "            # Handle datetime objects\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, set):\n",
    "            # Handle sets\n",
    "            return list(obj)\n",
    "        # Add more custom conversions for other data types if needed\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "with open(\"../resources/all_p_vals/p_vals_micro_tests_RQ1.json\", \"w\") as f:\n",
    "    json.dump(p_val, f, indent=3, cls=CustomJSONEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pass@1, temp 0.0 over different data averaged across tests for different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dataset_vs_format_on_original_data = all_data[(\n",
    "    all_data[\"TableManipulation\"] == \"OriginalData\") & (all_data[\"temperature\"] == 0.0)]\n",
    "dataset_vs_format_on_original_data = all_data_dataset_vs_format_on_original_data.pivot_table(\n",
    "    index=[\"Dataset-name\"],\n",
    "    columns='tableFormat',\n",
    "    values=['pass_1'],\n",
    "    aggfunc=[\"mean\"]  # Custom aggregation function to append values to a list\n",
    ")\n",
    "test_vs_format_on_original_data = np.round(\n",
    "    dataset_vs_format_on_original_data, decimals=2)\n",
    "test_vs_format_on_original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All noise vs test across formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"tableFormat\"].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_test_vs_format_on_all_noise_ops = all_data[(\n",
    "    all_data[\"temperature\"] == 0.0)]\n",
    "test_vs_format_on_all_noise_ops = all_data_test_vs_format_on_all_noise_ops.pivot_table(\n",
    "    index=[\"tableFormat\", \"TableManipulation\"],\n",
    "    columns='TestCase',\n",
    "    values=['pass_1'],\n",
    "    aggfunc=[\"mean\"]  # Custom aggregation function to append values to a list\n",
    ")\n",
    "test_vs_format_on_all_noise_ops_list = all_data_test_vs_format_on_all_noise_ops.pivot_table(\n",
    "    index=[\"tableFormat\", \"TableManipulation\"],\n",
    "    columns='TestCase',\n",
    "    values=['pass_1'],\n",
    "    # Custom aggregation function to append values to a list\n",
    "    aggfunc=lambda x: list(x)\n",
    ")\n",
    "test_vs_format_on_all_noise_ops = np.round(\n",
    "    test_vs_format_on_all_noise_ops, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = [\n",
    "    'OriginalData',\n",
    "    'ShuffleRows',\n",
    "    'ShuffleColumns',\n",
    "    'ShuffleColumnNames',\n",
    "    'SequentialColumnNames',\n",
    "    'ArbitraryColumnNames',\n",
    "    'TransposeTable',\n",
    "    'ColumnCluster',\n",
    "    'SerializeTable']\n",
    "table_formats = all_data[\"tableFormat\"].value_counts().index\n",
    "indi = [(formatType, noise)\n",
    "        for formatType in table_formats for noise in noises]\n",
    "cols = [(metric, \"pass_1\", test)\n",
    "        for test in attributes[\"TestCase\"] for metric in [\"mean\"]]\n",
    "cols_agg = [(\"pass_1\", test) for test in attributes[\"TestCase\"]]\n",
    "test_vs_format_on_all_noise_ops = test_vs_format_on_all_noise_ops[cols]\n",
    "test_vs_format_on_all_noise_ops = test_vs_format_on_all_noise_ops.reindex(indi)\n",
    "test_vs_format_on_all_noise_ops_list = test_vs_format_on_all_noise_ops_list[cols_agg]\n",
    "test_vs_format_on_all_noise_ops_list = test_vs_format_on_all_noise_ops_list.reindex(\n",
    "    indi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2: Delta values from original with p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "test_vs_format_on_all_noise_ops_diff_with_pval = test_vs_format_on_all_noise_ops.copy(\n",
    "    deep=True)\n",
    "test_vs_format_on_all_noise_ops_pval = test_vs_format_on_all_noise_ops.copy(\n",
    "    deep=True)\n",
    "for i in test_vs_format_on_all_noise_ops_diff_with_pval.index:\n",
    "    for j in test_vs_format_on_all_noise_ops_diff_with_pval.columns:\n",
    "        ix = (i[0], \"OriginalData\")\n",
    "        jj = (j[1], j[2])\n",
    "        original_values = test_vs_format_on_all_noise_ops_list.loc[ix, jj]\n",
    "        noise_induced_values = test_vs_format_on_all_noise_ops_list.loc[i, jj]\n",
    "\n",
    "        if i[1] != \"OriginalData\":\n",
    "            min_val = min(len(original_values), len(noise_induced_values))\n",
    "            p_value = scipy.stats.ttest_rel(\n",
    "                original_values[:min_val], noise_induced_values[:min_val])\n",
    "            benoffi_corrected_p_val_threshold = 0.01/8\n",
    "            subtract_from = test_vs_format_on_all_noise_ops.loc[ix, j]\n",
    "            difference = test_vs_format_on_all_noise_ops_diff_with_pval.loc[i,\n",
    "                                                                            j]-subtract_from\n",
    "            if \"-\" in str(difference):\n",
    "                str_diff = \"{:.2f}\".format(difference)\n",
    "            else:\n",
    "                str_diff = \"+\"+\"{:.2f}\".format(difference)\n",
    "            if p_value.pvalue < benoffi_corrected_p_val_threshold:\n",
    "                str_diff += \"**\"\n",
    "            test_vs_format_on_all_noise_ops_diff_with_pval.loc[i, j] = str_diff\n",
    "            test_vs_format_on_all_noise_ops_pval.loc[i, j] = p_value.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops_diff_with_pval.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops_diff = test_vs_format_on_all_noise_ops.copy(\n",
    "    deep=True)\n",
    "for i in test_vs_format_on_all_noise_ops_diff.index:\n",
    "    for j in test_vs_format_on_all_noise_ops_diff.columns:\n",
    "        ix = (i[0], \"OriginalData\")\n",
    "        if i[1] != \"OriginalData\":\n",
    "            subtract_from = test_vs_format_on_all_noise_ops.loc[ix, j]\n",
    "            difference = subtract_from - \\\n",
    "                test_vs_format_on_all_noise_ops_diff.loc[i, j]\n",
    "            if \"-\" in str(difference):\n",
    "                str_diff = \"{:.2f}\".format(difference)\n",
    "            else:\n",
    "                str_diff = \"+\"+\"{:.2f}\".format(difference)\n",
    "\n",
    "            test_vs_format_on_all_noise_ops_diff.loc[i, j] = str_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops_diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
