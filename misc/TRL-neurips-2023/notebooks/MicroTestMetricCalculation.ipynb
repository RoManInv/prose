{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd ../code_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1693566160494
        }
      },
      "outputs": [],
      "source": [
        "from tableTesting import *\n",
        "import pandas as pd\n",
        "import os\n",
        "import ast\n",
        "from itertools import combinations\n",
        "from typing import Any\n",
        "import json\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def metric_pass_k(expected_answer, answer: Any, k: int) -> float:\n",
        "    if not isinstance(answer, list):\n",
        "        raise ValueError(\"Answer should be a list for pass@k metric.\")\n",
        "\n",
        "    # Convert the test.expect to a list if it's not already\n",
        "    expected_values = expected_answer if isinstance(\n",
        "        expected_answer, list) else [expected_answer]\n",
        "    expected_values = list(map(str, expected_values))\n",
        "    boolean_answers = [True if answer[i]\n",
        "                       in expected_values else False for i in range(len(answer))]\n",
        "    combinations_k = list(combinations(boolean_answers, k))\n",
        "    passed_at_k = 0\n",
        "    # Calculate the pass@k metric\n",
        "    for comb in combinations_k:\n",
        "        if any(comb):\n",
        "            passed_at_k += 1\n",
        "    pass_at_k_percentage = (passed_at_k / len(combinations_k))*100\n",
        "\n",
        "    return pass_at_k_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bool_results(expected_answer, answer: Any) -> float:\n",
        "    if not isinstance(answer, list):\n",
        "        raise ValueError(\"Answer should be a list for pass@k metric.\")\n",
        "\n",
        "    # Convert the test.expect to a list if it's not already\n",
        "    expected_values = expected_answer if isinstance(\n",
        "        expected_answer, list) else [expected_answer]\n",
        "    expected_values = list(map(str, expected_values))\n",
        "    if answer[0] in expected_values:\n",
        "        return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check(self, test, answer: Any) -> bool:\n",
        "    if test.TestType in [\"ColumnLookupTests\", \"RowLookupTests\"]:\n",
        "        if isinstance(test.expect, list):\n",
        "            expected_values = test.expect if isinstance(\n",
        "                test.expect, list) else [test.expect]\n",
        "            expected_values = list(map(str, expected_values))\n",
        "            matches = set(expected_values).intersection(set([answer]))\n",
        "        else:\n",
        "            print(\"Error: The expected Answer 'List' should be a list not string\")\n",
        "        return len(matches) > 0\n",
        "    return str(test.expect) == answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark_data_path = \"../resources/BenchmarkDatasetClean/\"\n",
        "benchmark_dataset = os.listdir(benchmark_data_path)\n",
        "benchmark_save_path = \"../resources/BenchmarkResults31_8_2023/\"\n",
        "save_path_plot = \"../resources/BenchmarkResults31_8_2023_plots/\"\n",
        "if not os.path.exists(save_path_plot):\n",
        "    os.makedirs(save_path_plot)\n",
        "sp = \"../BenchmarkFinalResults_31_8_2023/\"\n",
        "if not os.path.exists(sp):\n",
        "    os.makedirs(sp)\n",
        "spa = \"../BenchmarkFinalArrangedResults_31_8_2023/\"\n",
        "if not os.path.exists(spa):\n",
        "    os.makedirs(spa)\n",
        "benchmark_dataset = [\n",
        "    data for data in benchmark_dataset if data.endswith(\".csv\")]\n",
        "for bd in benchmark_dataset:\n",
        "    try:\n",
        "        pass_metrics = []\n",
        "        t_F_bools = []\n",
        "        print(bd)\n",
        "        df = pd.read_csv(os.path.join(\n",
        "            benchmark_save_path, bd, bd+\"_output.csv\"))\n",
        "        print(df.shape)\n",
        "        dirs = os.listdir(os.path.join(\n",
        "            benchmark_save_path, bd, \"cache_logger\"))\n",
        "        dirs = [d for d in dirs if d.endswith(\".jsonl\")]\n",
        "        print(dirs)\n",
        "        with open(os.path.join(benchmark_save_path, bd, \"cache_logger\", dirs[-1]), \"r\") as file:\n",
        "            for line in file:\n",
        "                line = ast.literal_eval(line.strip())\n",
        "                if line.startswith(\"\\\"\") and line.endswith(\"\\\"\"):\n",
        "                    line = line[1:-1]\n",
        "                data_ = json.loads((line))\n",
        "                expected_ans = data_[\"test-expected-answer\"]\n",
        "                ans = data_[\"answer\"]\n",
        "                try:\n",
        "                    pass_metrics.append(\n",
        "                        [metric_pass_k(expected_ans, ans, k) for k in [1, 3, 5, 10, 15]])\n",
        "                    t_F_bools.append(bool_results(expected_ans, ans))\n",
        "                except Exception as err:\n",
        "                    print(f\"error {err} catched during metric calculation\")\n",
        "                    pass_metrics.append([None for k in [1, 3, 5, 10, 15]])\n",
        "                    t_F_bools.append(False)\n",
        "                    pass\n",
        "        print(len(pass_metrics))\n",
        "        df[['pass_1', 'pass_3', 'pass_5', 'pass_10', 'pass_15']] = pass_metrics\n",
        "        df[\"T/F\"] = t_F_bools\n",
        "        print(df.columns)\n",
        "        df.to_csv(os.path.join(benchmark_save_path, bd, bd +\n",
        "                  \"_output_revamped_metrics_with_bools.csv\"), index=None)\n",
        "\n",
        "    except Exception as Err:\n",
        "        print(\"ERROR------------->\", Err)\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
