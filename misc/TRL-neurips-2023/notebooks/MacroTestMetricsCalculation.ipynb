{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../code_\n",
    "from collections import Counter\n",
    "from tableTestingNewMacro import *\n",
    "from utils import Convert_back_to_df\n",
    "from compare import compare_per_cell\n",
    "import pandas as pd\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "from typing import Any, List\n",
    "from itertools import combinations\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(expected_df: pd.DataFrame, answer: pd.DataFrame) -> bool:\n",
    "    expected_df = expected_df.astype(str)\n",
    "    try:\n",
    "        if expected_df.shape == answer.shape:\n",
    "            return expected_df.equals(answer)\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def metric_pass_k(expected_df: pd.DataFrame, answer: List[pd.DataFrame], k: int) -> float:\n",
    "    expected_df = expected_df.astype(str)\n",
    "    boolean_answers = [check(expected_df, answer[i])\n",
    "                       for i in range(len(answer))]\n",
    "    combinations_k = list(combinations(boolean_answers, k))\n",
    "    passed_at_k = 0\n",
    "    # Calculate the pass@k metric\n",
    "    for comb in combinations_k:\n",
    "        if any(comb):\n",
    "            passed_at_k += 1\n",
    "    pass_at_k_percentage = (passed_at_k / len(combinations_k))*100\n",
    "\n",
    "    return pass_at_k_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_it(expected_df, format_name):\n",
    "    if format_name == \"JsonFormat\":\n",
    "        return str(JsonFormat().formatting(next(SerializeTable().modify(expected_df))))\n",
    "    if format_name == \"DFloaderFormat\":\n",
    "        return str(DFloaderFormat().formatting(next(SerializeTable().modify(expected_df))))\n",
    "    if format_name == \"DataMatrixFormat\":\n",
    "        return str(DataMatrixFormat().formatting(next(SerializeTable().modify(expected_df))))\n",
    "    if format_name == \"MarkdownFormat\":\n",
    "        return str(MarkdownFormat().formatting(next(SerializeTable().modify(expected_df))))\n",
    "    if format_name == \"CommaSeparatedFormat\":\n",
    "        return str(CommaSeparatedFormat().formatting(next(SerializeTable().modify(expected_df))))\n",
    "    if format_name == \"TabSeparatedFormat\":\n",
    "        return str(TabSeparatedFormat().formatting(next(SerializeTable().modify(expected_df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_data_path = \"../resources/BenchmarkDataset\"\n",
    "benchmark_dataset = os.listdir(benchmark_data_path)\n",
    "# 3----> macro test -noises 8 formats 3 test\n",
    "benchmark_save_path = \"../resources/BenchmarkResults_29_9_2023_Macro_Tests_all_noise_redo_redo/\"\n",
    "benchmark_dataset = [d for d in benchmark_dataset if d.endswith(\".csv\")]\n",
    "sp = spa = benchmark_save_path\n",
    "\n",
    "for bd in benchmark_dataset:\n",
    "    try:\n",
    "        pass_metrics = []\n",
    "        t_F_bools = []\n",
    "        scores_all_precision = []\n",
    "        scores_all_recall = []\n",
    "        print(bd)\n",
    "        df = pd.read_csv(os.path.join(benchmark_save_path,\n",
    "                         bd, \"Macro_\"+bd+\"_output.csv\"))\n",
    "        print(df.shape)\n",
    "        dirs = os.listdir(os.path.join(\n",
    "            benchmark_save_path, bd, \"cache_logger_new\"))\n",
    "        print(dirs)\n",
    "        error_lines = 0\n",
    "        error_converts = 0\n",
    "        found = 0\n",
    "        table_ops = []\n",
    "        error_converts_format = []\n",
    "        ops_candidate_error = []\n",
    "        test_candidate_error = []\n",
    "        error_logs = []\n",
    "        print(\"use:\", dirs[-1])\n",
    "        with open(os.path.join(benchmark_save_path, bd, \"cache_logger_new\", dirs[-1]), \"r\") as file:\n",
    "            for line in tqdm.tqdm(file):\n",
    "                try:\n",
    "                    data_ = json.loads((line))\n",
    "                    expected_ans = data_[\"test-expected-answer\"]\n",
    "                    expected_df = Convert_back_to_df(\n",
    "                        \"JsonFormat\", str(expected_ans))\n",
    "                    gen = data_[\"gen\"]\n",
    "                    if gen == \"TableReconstructionTests\" and data_[\"table_op\"] == \"SampleRows\":\n",
    "                        if serialize_it(expected_df, data_[\"tab_format\"]) in data_[\"prompt_cache_per_call\"][\"prompt\"]:\n",
    "                            table_op = \"SerializeTable\"\n",
    "                            found += 1\n",
    "                    elif gen == \"TableReconstructionTests1\":\n",
    "                        table_op = \"SerializeTable\"\n",
    "                    else:\n",
    "                        table_op = data_[\"table_op\"]\n",
    "                    table_ops.append(table_op)\n",
    "                    answer = data_[\"answer\"]\n",
    "                    tab_format = data_[\"tab_format\"]\n",
    "                    answer_changed_format = []\n",
    "                    for a in answer:\n",
    "                        try:\n",
    "                            conv = Convert_back_to_df(tab_format, a)\n",
    "                        except Exception as err:\n",
    "                            conv = pd.DataFrame()\n",
    "                            error_converts += 1\n",
    "                            if a == answer[0]:\n",
    "                                error_converts_format.append(\n",
    "                                    data_[\"tab_format\"])\n",
    "\n",
    "                        answer_changed_format.append(conv)\n",
    "\n",
    "                    a = answer_changed_format[0]\n",
    "                    try:\n",
    "                        scores_all_precision.append(\n",
    "                            compare_per_cell(a, expected_df)*100)\n",
    "                    except:\n",
    "                        scores_all_precision.append(0)\n",
    "                    try:\n",
    "                        # , type_reference_df=a, count_header_and_index=True, return_fraction=False))\n",
    "                        scores_all_recall.append(\n",
    "                            compare_per_cell(expected_df, a)*100)\n",
    "                    except:\n",
    "                        scores_all_recall.append(0)\n",
    "\n",
    "                    # break\n",
    "                    try:\n",
    "                        pass_metrics.append([metric_pass_k(\n",
    "                            expected_ans, answer_changed_format, k) for k in [1, 3, 5, 10, 15]])\n",
    "                    except Exception as Err:\n",
    "                        pass_metrics.append([None for k in [1, 3, 5, 10, 15]])\n",
    "                    try:\n",
    "                        t_F_bools.append(\n",
    "                            check(expected_ans, answer_changed_format[0]))\n",
    "\n",
    "                    except Exception as Err:\n",
    "                        t_F_bools.append(False)\n",
    "                except Exception as err:\n",
    "\n",
    "                    ops_candidate_error.append(data_[\"table_op\"])\n",
    "                    test_candidate_error.append(data_[\"gen\"])\n",
    "                    error_logs.append(data_[\"error\"])\n",
    "                    error_lines += 1\n",
    "                    scores_all_precision.append(0)\n",
    "                    scores_all_recall.append(0)\n",
    "                    t_F_bools.append(False)\n",
    "                    pass_metrics.append([None for k in [1, 3, 5, 10, 15]])\n",
    "\n",
    "        print(len(pass_metrics))\n",
    "        print(Counter(ops_candidate_error).items())\n",
    "        print(Counter(test_candidate_error).items())\n",
    "        print(Counter(error_logs).items())\n",
    "        print(\"NO of error lines:\", error_lines)\n",
    "        print(\"error_converts:\", error_converts/15, error_converts)\n",
    "        print(\"found serialized:\", found)\n",
    "        print(Counter(table_ops).items())\n",
    "        print(\"failed format counts:\", Counter(error_converts_format).items())\n",
    "        df[\"table_ops\"] = table_ops\n",
    "        df[['pass_1', 'pass_3', 'pass_5', 'pass_10', 'pass_15']] = pass_metrics\n",
    "        df[\"Result\"] = df[\"Result\"].apply(lambda x: 1 if x else 0)\n",
    "        df[\"T/F\"] = t_F_bools\n",
    "        df[\"precision_per_cell_correctness_top1\"] = scores_all_precision\n",
    "        df[\"recall_per_cell_correctness_top1\"] = scores_all_recall\n",
    "        df.to_csv(os.path.join(benchmark_save_path, bd,\n",
    "                  \"Macro_\"+bd+\"_output_revamped.csv\"), index=None)\n",
    "\n",
    "        print(f\"{'-'*90}\\n\")\n",
    "    except Exception as Err:\n",
    "        print(\"error encountered\")\n",
    "        print(Err)\n",
    "        pass\n",
    "    print(f\"{'-'*90}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
