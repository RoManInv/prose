{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.contingency_tables as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_data_path = \"../resources/BenchmarkDatasetClean/\"\n",
    "benchmark_dataset = os.listdir(benchmark_data_path)\n",
    "benchmark_save_path = \"../resources/BenchmarkResults_29_9_2023_Macro_Tests_all_noise_redo_redo\"\n",
    "benchmark_save_path2 = \"../resources/BenchmarkResults_23_9_2023_Macro_Tests_HTML_original\"\n",
    "benchmark_save_path1 = \"../resources/BenchmarkResults_23_9_2023_Macro_Tests_new_format_added_test_redo/\"\n",
    "\n",
    "benchmark_dataset = [data for data in benchmark_dataset if data.endswith(\n",
    "    \".csv\") and \"breast\" not in data]\n",
    "dfs = []\n",
    "for bd in benchmark_dataset:\n",
    "\n",
    "    pass_metrics = []\n",
    "    print(bd)\n",
    "    df1 = pd.read_csv(os.path.join(benchmark_save_path1, bd,\n",
    "                      \"Macro_\"+bd+\"_output_revamped.csv\"))\n",
    "    df2 = pd.read_csv(os.path.join(benchmark_save_path2, bd,\n",
    "                      \"Macro_\"+bd+\"_output_revamped.csv\"))\n",
    "    df3 = pd.read_csv(os.path.join(benchmark_save_path, bd,\n",
    "                      \"Macro_\"+bd+\"_output_revamped.csv\"))\n",
    "    df2[\"TableManipulation\"] = \"OriginalData\"\n",
    "    df1[\"TableManipulation\"] = \"OriginalData\"\n",
    "    df2[\"precision_per_cell_correctness_top1\"] = df2[\"precision_per_cell_correctness_top1\"]*0.01\n",
    "    df2[\"recall_per_cell_correctness_top1\"] = df2[\"recall_per_cell_correctness_top1\"]*0.01\n",
    "    df3[\"precision_per_cell_correctness_top1\"] = df3[\"precision_per_cell_correctness_top1\"]*0.01\n",
    "    df3[\"recall_per_cell_correctness_top1\"] = df3[\"recall_per_cell_correctness_top1\"]*0.01\n",
    "    df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "    df[\"Dataset-name\"] = bd\n",
    "    df[\"F1Score_per_cell_correctness_top1\"] = df.apply(lambda x: np.round(2*(x.recall_per_cell_correctness_top1*x.precision_per_cell_correctness_top1)/(\n",
    "        x.recall_per_cell_correctness_top1+x.precision_per_cell_correctness_top1)) if (x.recall_per_cell_correctness_top1*x.precision_per_cell_correctness_top1) != 0 else 0, axis=1)\n",
    "    dfs.append(df)\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "attributes = {c: list(df[c].unique()) for c in df.columns if c in [\n",
    "    \"temperature\", \"tableFormat\", \"table_ops\", \"TestCase\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RQ3: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average percell, @pass 1 formats- noise- vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = ['TableColumnReorderTests',\n",
    "              'TableReconstructionTests1',\n",
    "              'TableTransposeTests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_test_vs_format_on_all_noise_ops = all_data[(\n",
    "    all_data[\"temperature\"] == 0.0)] \n",
    "print(all_data_test_vs_format_on_all_noise_ops.shape)\n",
    "test_vs_format_on_all_noise_ops = all_data_test_vs_format_on_all_noise_ops.pivot_table(\n",
    "    index=[\"tableFormat\", \"TableManipulation\"],\n",
    "    columns='TestCase',\n",
    "    values=[\"F1Score_per_cell_correctness_top1\", 'recall_per_cell_correctness_top1',\n",
    "            \"precision_per_cell_correctness_top1\"],  \n",
    "    aggfunc=[\"mean\"]  # Custom aggregation function to append values to a list\n",
    ")\n",
    "test_vs_format_on_all_noise_ops = test_vs_format_on_all_noise_ops*100\n",
    "test_vs_format_on_all_noise_ops = np.round(\n",
    "    test_vs_format_on_all_noise_ops, decimals=2)\n",
    "print(test_vs_format_on_all_noise_ops.columns)\n",
    "noises = [\n",
    "    'OriginalData',\n",
    "    'ShuffleRows',\n",
    "    'ShuffleColumns',\n",
    "    'ShuffleColumnNames',\n",
    "    'SequentialColumnNames',\n",
    "    'ArbitraryColumnNames',\n",
    "    'TransposeTable',\n",
    "    'ColumnCluster',\n",
    "    'SerializeTable']\n",
    "table_formats = all_data[\"tableFormat\"].value_counts().index\n",
    "indi = [(formatType, noise)\n",
    "        for formatType in table_formats for noise in noises]\n",
    "cols = [(\"mean\", f\"{metric}_per_cell_correctness_top1\", T)\n",
    "        for T in test_cases for metric in [\"precision\", \"recall\", \"F1Score\"]]\n",
    "test_vs_format_on_all_noise_ops = test_vs_format_on_all_noise_ops[cols]\n",
    "test_vs_format_on_all_noise_ops = test_vs_format_on_all_noise_ops.reindex(indi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops.tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diff with p_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops_list = all_data_test_vs_format_on_all_noise_ops.pivot_table(\n",
    "    index=[\"tableFormat\", \"TableManipulation\"],\n",
    "    columns='TestCase',\n",
    "    values=[\"F1Score_per_cell_correctness_top1\", 'recall_per_cell_correctness_top1',\n",
    "            \"precision_per_cell_correctness_top1\"],  \n",
    "    # Custom aggregation function to append values to a list\n",
    "    aggfunc=lambda x: list(x)\n",
    ")\n",
    "\n",
    "cols_agg = [(f\"{metric}_per_cell_correctness_top1\", T)\n",
    "            for T in test_cases for metric in [\"precision\", \"recall\", \"F1Score\"]]\n",
    "\n",
    "test_vs_format_on_all_noise_ops_list = test_vs_format_on_all_noise_ops_list[cols_agg]\n",
    "test_vs_format_on_all_noise_ops_list = test_vs_format_on_all_noise_ops_list.reindex(\n",
    "    indi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "test_vs_format_on_all_noise_ops_diff_with_pval = test_vs_format_on_all_noise_ops.copy(\n",
    "    deep=True)\n",
    "test_vs_format_on_all_noise_ops_pval = test_vs_format_on_all_noise_ops.copy(\n",
    "    deep=True)\n",
    "for i in test_vs_format_on_all_noise_ops_diff_with_pval.index:\n",
    "    for j in test_vs_format_on_all_noise_ops_diff_with_pval.columns:\n",
    "        ix = (i[0], \"OriginalData\")\n",
    "        jj = (j[1], j[2])\n",
    "        original_values = test_vs_format_on_all_noise_ops_list.loc[ix, jj]\n",
    "        noise_induced_values = test_vs_format_on_all_noise_ops_list.loc[i, jj]\n",
    "\n",
    "        if i[1] != \"OriginalData\":\n",
    "            min_val = min(len(original_values), len(noise_induced_values))\n",
    "\n",
    "            p_value = scipy.stats.ttest_rel(\n",
    "                original_values[:min_val], noise_induced_values[:min_val])\n",
    "\n",
    "            benoffi_corrected_p_val_threshold = 0.01/8\n",
    "            subtract_from = test_vs_format_on_all_noise_ops.loc[ix, j]\n",
    "            difference = test_vs_format_on_all_noise_ops_diff_with_pval.loc[i,\n",
    "                                                                            j]-subtract_from\n",
    "            if \"-\" in str(difference):\n",
    "                str_diff = \"{:.2f}\".format(difference)\n",
    "            else:\n",
    "                str_diff = \"+\"+\"{:.2f}\".format(difference)\n",
    "            if p_value.pvalue < benoffi_corrected_p_val_threshold:\n",
    "\n",
    "                str_diff += \"**\"\n",
    "            test_vs_format_on_all_noise_ops_diff_with_pval.loc[i, j] = str_diff\n",
    "            test_vs_format_on_all_noise_ops_pval.loc[i, j] = p_value.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops_diff_with_pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only diff Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops_diff = test_vs_format_on_all_noise_ops.copy(\n",
    "    deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_vs_format_on_all_noise_ops_diff.index:\n",
    "    for j in test_vs_format_on_all_noise_ops_diff.columns:\n",
    "        ix = (i[0], \"OriginalData\")\n",
    "        if i[1] != \"OriginalData\":\n",
    "            subtract_from = test_vs_format_on_all_noise_ops.loc[ix, j]\n",
    "            difference = subtract_from - \\\n",
    "                test_vs_format_on_all_noise_ops_diff.loc[i, j]\n",
    "            if \"-\" in str(difference):\n",
    "                str_diff = \"{:.2f}\".format(difference)\n",
    "            else:\n",
    "                str_diff = \"+\"+\"{:.2f}\".format(difference)\n",
    "\n",
    "            test_vs_format_on_all_noise_ops_diff.loc[i, j] = str_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_format_on_all_noise_ops_diff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
