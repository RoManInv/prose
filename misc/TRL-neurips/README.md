# General Guideline
This folder contains evaluation code and results associated with the paper[**Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs**](https://openreview.net/forum?id=Ld5UCpiT07) to appear at
[TRL@NeurIPS 2023](https://table-representation-learning.github.io/)

## Setup
Assuming you want to setup inside a Python virtual environment.

```powershell
python -m virtualenv venv/
venv/scripts/activate
pip install -r requirements.txt
```

You should have your OPENAI key as an environment variable, because we use the key to access the GPT3.5 `text-davinci-003 model`

```
$env:OPENAI_API_KEY=...
```
## Resources
Inside the `resources` folder you will find `\BenchmarkDataset` subfolder which consists of seven datasets from kaggle. These seven datasets are used as benchmarks to evaluate LLM's understanding through our self-supervised evalution setup.

In order to access the Benchmark results that we obtained, we request you to download the `BenchmarkResults.zip` from [TodO](). Make sure to unzip the file at root of `TRL-neurips\resources\` folder for sucessful code runs.

## Code Usage Guideline

In the paper we talk introduce Self-Supervised Structural Tasks to evaluate the performance of the LLM.

For the fact-finding task you can refer the `.\code_\tableTestingMicroScript.py`. A notebook `.\notebooks\MicroTableTesting.ipynb` is also implemented to how our fact-finding evaluation suit is used.

For the table-transformation task you can refer the `code_\tableTestingMacroScript.py`. A notebook `.\notebooks\MacroTableTesting.ipynb` is also implemented to how our table-transformation evaluation suit is used.

## Metrics

###  Fact-finding tasks
We report the average pass@1 and p-value for the fact-finding tasks.

`.\notebooks\MicroTestMetricsCalculation.ipynb` takes the log file generated by the `.\notebooks\MicroTableTesting.ipynb`file and does some pre-requite metric calculation and finally generate the final csv files. 

The final csv files are then used by `.\notebooks\MicroTestResults.ipynb` to provide the pass@1 scores on the Original data across all the fact-finding tasks, along with p-value and the average pass@1 delta from original to noisy operation.

### Table-transformation tasks
We report the average F1 score and p-value for the table-transformation tasks. 

`.\notebooks\MacroTestMetricsCalculation.ipynb` takes the log file generated by the `.\notebooks\MacroTableTesting.ipynb` file and does some pre-requite metric calculation and finally generate the final csv files.

The final csv files are then used by `.\notebooks\MacroTestResults_RQ1.ipynb` to provide the F1 scores on the Original data across all the fact-finding tasks, along with p-value. Whereas `.\notebooks\MacroTestResults_RQ2.ipynb` reports average F1 score delta from original to noisy for transformation tasks along with the p-values.

# Citation

If you find our work useful in your research, please consider citing the paper:

@inproceedings{singha2023tabular,
  title={Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs},
  author={Singha, Ananya and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Parnin, Chris},
  booktitle={NeurIPS 2023 Second Table Representation Learning Workshop},
  year={2023}
}

# Contact
For any questions or issues, please submit repository issues or reach us via email at `t-asingha@microsoft.com`