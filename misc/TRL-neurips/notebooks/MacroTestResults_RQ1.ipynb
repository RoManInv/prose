{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.contingency_tables as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_data_path = \"../resources/BenchmarkDatasetClean/\"\n",
    "benchmark_dataset = os.listdir(benchmark_data_path)\n",
    "benchmark_save_path = \"../resources/BenchmarkResults_29_9_2023_Macro_Tests_all_noise_redo_redo\"\n",
    "benchmark_save_path2 = \"../resources/BenchmarkResults_23_9_2023_Macro_Tests_HTML_original\"\n",
    "benchmark_save_path1 = \"../resources/BenchmarkResults_23_9_2023_Macro_Tests_new_format_added_test_redo/\"\n",
    "\n",
    "benchmark_dataset = [data for data in benchmark_dataset if data.endswith(\n",
    "    \".csv\") and \"breast\" not in data]\n",
    "dfs = []\n",
    "for bd in benchmark_dataset:\n",
    "\n",
    "    pass_metrics = []\n",
    "    print(bd)\n",
    "    df1 = pd.read_csv(os.path.join(benchmark_save_path1, bd,\n",
    "                      \"Macro_\"+bd+\"_output_revamped.csv\"))\n",
    "    df2 = pd.read_csv(os.path.join(benchmark_save_path2, bd,\n",
    "                      \"Macro_\"+bd+\"_output_revamped.csv\"))\n",
    "    df3 = pd.read_csv(os.path.join(benchmark_save_path, bd,\n",
    "                      \"Macro_\"+bd+\"_output_revamped.csv\"))\n",
    "    df2[\"precision_per_cell_correctness_top1\"] = df2[\"precision_per_cell_correctness_top1\"]*0.01\n",
    "    df2[\"recall_per_cell_correctness_top1\"] = df2[\"recall_per_cell_correctness_top1\"]*0.01\n",
    "    df3[\"precision_per_cell_correctness_top1\"] = df3[\"precision_per_cell_correctness_top1\"]*0.01\n",
    "    df3[\"recall_per_cell_correctness_top1\"] = df3[\"recall_per_cell_correctness_top1\"]*0.01\n",
    "    df2[\"TableManipulation\"] = \"OriginalData\"\n",
    "    df1[\"TableManipulation\"] = \"OriginalData\"\n",
    "    df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "    print(df1.shape, df2.shape, df.shape)\n",
    "    df[\"Dataset-name\"] = bd\n",
    "    df[\"F1Score_per_cell_correctness_top1\"] = df.apply(lambda x: np.round(2*(x.recall_per_cell_correctness_top1*x.precision_per_cell_correctness_top1)/(\n",
    "        x.recall_per_cell_correctness_top1+x.precision_per_cell_correctness_top1)) if (x.recall_per_cell_correctness_top1*x.precision_per_cell_correctness_top1) != 0 else 0, axis=1)\n",
    "    dfs.append(df)\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "attributes = {c: list(df[c].unique()) for c in df.columns if c in [\n",
    "    \"temperature\", \"tableFormat\", \"table_ops\", \"TestCase\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RQ1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\"TableColumnReorderTests\",\n",
    "              \"TableReconstructionTests1\", \"TableTransposeTests\"]\n",
    "formats = all_data[\"tableFormat\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_test_vs_format_on_all_noise_ops = all_data[(\n",
    "    all_data[\"temperature\"] == 0.0)]  \n",
    "print(all_data_test_vs_format_on_all_noise_ops.shape)\n",
    "test_vs_format_on_all_noise_ops = all_data_test_vs_format_on_all_noise_ops.pivot_table(\n",
    "    index=[\"tableFormat\"],\n",
    "    columns='TestCase',\n",
    "    values=[\"F1Score_per_cell_correctness_top1\", 'recall_per_cell_correctness_top1',\n",
    "            \"precision_per_cell_correctness_top1\"],  \n",
    "    aggfunc=[\"mean\"]  # Custom aggregation function to append values to a list\n",
    ")\n",
    "test_vs_format_on_all_noise_ops = test_vs_format_on_all_noise_ops*100\n",
    "test_vs_format_on_all_noise_ops = np.round(\n",
    "    test_vs_format_on_all_noise_ops, decimals=2)\n",
    "cols = [(\"mean\", f\"{metric}_per_cell_correctness_top1\", T)\n",
    "        for T in test_cases for metric in [\"precision\", \"recall\", \"F1Score\"]]\n",
    "test_vs_format_on_all_noise_ops = test_vs_format_on_all_noise_ops[cols]\n",
    "indi = ['CommaSeparatedFormat', 'DFloaderFormat', 'DataMatrixFormat',\n",
    "        'JsonFormat', 'MarkdownFormat',\n",
    "        'TabSeparatedFormat', 'HTMLFormat', 'HTMLNoSpaceFormat']\n",
    "test_vs_format_on_all_noise_ops = test_vs_format_on_all_noise_ops.reindex(indi)\n",
    "test_vs_format_on_all_noise_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_f1 = [(\"mean\", f\"{metric}_per_cell_correctness_top1\", T)\n",
    "           for T in test_cases for metric in [\"F1Score\"]]\n",
    "test_vs_format_on_all_noise_ops_f1 = test_vs_format_on_all_noise_ops[cols_f1]\n",
    "test_vs_format_on_all_noise_ops_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P-values RQ1 Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_test_vs_format_on_all_noise_ops = all_data[(\n",
    "    all_data[\"temperature\"] == 0.0)]  \n",
    "print(all_data_test_vs_format_on_all_noise_ops.shape)\n",
    "test_vs_format_on_all_noise_ops = all_data_test_vs_format_on_all_noise_ops.pivot_table(\n",
    "    index=[\"TestCase\"],\n",
    "    columns='tableFormat',\n",
    "    values=[\"F1Score_per_cell_correctness_top1\"],\n",
    "    # Custom aggregation function to append values to a list\n",
    "    aggfunc=lambda x: list(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "p_val = {}\n",
    "for id in range(test_vs_format_on_all_noise_ops.shape[0]):\n",
    "    print(id)\n",
    "    if test_vs_format_on_all_noise_ops.index[id] != \"TableReconstructionTests\":\n",
    "        print(\n",
    "            f\"For Test: {test_vs_format_on_all_noise_ops.index[id]} pass@1 temp =0.1\")\n",
    "        sorted_list = [i for i in test_vs_format_on_all_noise_ops_f1.sort_values(\n",
    "            by=[(\"mean\", \"F1Score_per_cell_correctness_top1\", test_vs_format_on_all_noise_ops.index[id])], ascending=False).index]\n",
    "        print(sorted_list)\n",
    "        sub_vals = {}\n",
    "        index_val = test_vs_format_on_all_noise_ops.index[id]\n",
    "        for x in range(1, len(sorted_list)):\n",
    "            vals1 = test_vs_format_on_all_noise_ops.loc[index_val, (\n",
    "                \"F1Score_per_cell_correctness_top1\", sorted_list[0])]\n",
    "            vals2 = test_vs_format_on_all_noise_ops.loc[index_val, (\n",
    "                \"F1Score_per_cell_correctness_top1\", sorted_list[x])]\n",
    "            min_val = min(len(vals1), len(vals2))\n",
    "            print(min_val)\n",
    "            p_value = scipy.stats.ttest_rel(vals1[:min_val], vals2[:min_val])\n",
    "            sub_vals[f\"{sorted_list[0]}-{sorted_list[x]}\"] = {\"p-value\": p_value.pvalue,\n",
    "                                                              \"statistics\": p_value.statistic, \"df\": p_value.df, \"tests_count\": min_val}\n",
    "        p_val[test_vs_format_on_all_noise_ops.index[id]] = sub_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, pd.Series):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, datetime):\n",
    "            # Handle datetime objects\n",
    "            return obj.isoformat()\n",
    "        elif isinstance(obj, set):\n",
    "            # Handle sets\n",
    "            return list(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "with open(\"../resources/all_p_vals/p_vals_macro_tests_RQ1.json\", \"w\") as f:\n",
    "    json.dump(p_val, f, indent=3, cls=CustomJSONEncoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
